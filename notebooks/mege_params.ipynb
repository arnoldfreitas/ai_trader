{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRL_TRAININGS = {\n",
    "    \"reward_sharpe_ratio_0\": \"20230429_200721\",\n",
    "    \"reward_differential_sharpe_ratio_0\": \"20230424_070731\",\n",
    "    \"compute_reward_from_tutor_0\": \"20230429_200559\",\n",
    "    \"reward_profit_2\": \"20230420_083508\",\n",
    "}\n",
    "\n",
    "DQN_TRAININGS = {\n",
    "    \"reward_sharpe_ratio_0\": \"20230427_165557\",\n",
    "    \"reward_differential_sharpe_ratio_1\": \"20230423_114422\",\n",
    "    \"compute_reward_from_tutor_1\": \"20230421_181519\",\n",
    "    \"reward_profit_1\": \"20230423_231505\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = pathlib.Path(r'C:\\\\Users\\\\romy_\\\\Documents\\\\arnolduni\\\\ai_trader_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(path_to_folders, algorithm, trial=0):\n",
    "    trial_name = [x for x in os.listdir(path_to_folders) if os.path.isdir(os.path.join(path_to_folders, x))]\n",
    "    if trial_name:\n",
    "        trial_name = trial_name[0]\n",
    "    else:\n",
    "        trial_name = f\"{algorithm.upper()}_trial_{trial}\"\n",
    "\n",
    "    data = json.load(open(os.path.join(path_to_folders, trial_name, 'params.json')))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_list = list()\n",
    "for ele in DRL_TRAININGS.values():\n",
    "    data_folder = os.path.join(base_folder, ele)\n",
    "    tmp = get_params(data_folder, 'DRL')\n",
    "    dicts_list.append(tmp)\n",
    "\n",
    "for ele in DQN_TRAININGS.values():\n",
    "    data_folder = os.path.join(base_folder, ele)\n",
    "    tmp = get_params(data_folder, 'DQN')\n",
    "    dicts_list.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_domain</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>asset</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>data_source</th>\n",
       "      <th>episodes</th>\n",
       "      <th>loss_method</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>epsilon_decay</th>\n",
       "      <th>...</th>\n",
       "      <th>fee</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>money</th>\n",
       "      <th>obs_space</th>\n",
       "      <th>reward_function</th>\n",
       "      <th>runs_p_eps</th>\n",
       "      <th>action_space</th>\n",
       "      <th>trainer</th>\n",
       "      <th>from_checkpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DRL_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>reward_sharpe_ratio</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>DRLTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DRL_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>reward_differential_sharpe_ratio</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>DRLTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DRL_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>compute_reward_from_tutor</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>DRLTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DRL_trial_8</td>\n",
       "      <td>BTC</td>\n",
       "      <td>32</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>reward_profit</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>DRLTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>reward_sharpe_ratio</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>DQNTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>reward_differential_sharpe_ratio</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>DQNTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>compute_reward_from_tutor</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>DQNTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>BTC</td>\n",
       "      <td>16</td>\n",
       "      <td>BTC_histData_dt1800.0s_20220825_0629</td>\n",
       "      <td>50</td>\n",
       "      <td>stantard_loss</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>10000</td>\n",
       "      <td>[8, 20]</td>\n",
       "      <td>reward_profit</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>DQNTrainer</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  action_domain    algorithm asset  batch_size  \\\n",
       "0    [0.0, 1.0]  DRL_trial_0   BTC          16   \n",
       "1    [0.0, 1.0]  DRL_trial_0   BTC          16   \n",
       "2    [0.0, 1.0]  DRL_trial_0   BTC          16   \n",
       "3    [0.0, 1.0]  DRL_trial_8   BTC          32   \n",
       "4    [0.0, 1.0]  DQN_trial_0   BTC          16   \n",
       "5    [0.0, 1.0]  DQN_trial_0   BTC          16   \n",
       "6    [0.0, 1.0]  DQN_trial_0   BTC          16   \n",
       "7    [0.0, 1.0]  DQN_trial_0   BTC          16   \n",
       "\n",
       "                            data_source  episodes    loss_method  epoch  \\\n",
       "0  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "1  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "2  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "3  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "4  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "5  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "6  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "7  BTC_histData_dt1800.0s_20220825_0629        50  stantard_loss      2   \n",
       "\n",
       "   epsilon  epsilon_decay  ...    fee  gamma  learning_rate  money  obs_space  \\\n",
       "0      0.5           0.75  ...  0.001   0.95   1.000000e-07  10000    [8, 20]   \n",
       "1      0.5           0.75  ...  0.001   0.95   1.000000e-07  10000    [8, 20]   \n",
       "2      0.5           0.75  ...  0.001   0.95   1.000000e-06  10000    [8, 20]   \n",
       "3      0.5           0.75  ...  0.001   0.95   1.000000e-05  10000    [8, 20]   \n",
       "4      0.5           0.75  ...  0.001   0.95   1.000000e-07  10000    [8, 20]   \n",
       "5      0.5           0.75  ...  0.001   0.95   1.000000e-07  10000    [8, 20]   \n",
       "6      0.5           0.75  ...  0.001   0.95   1.000000e-03  10000    [8, 20]   \n",
       "7      0.5           0.75  ...  0.001   0.95   1.000000e-05  10000    [8, 20]   \n",
       "\n",
       "                    reward_function runs_p_eps  action_space     trainer  \\\n",
       "0               reward_sharpe_ratio          5             1  DRLTrainer   \n",
       "1  reward_differential_sharpe_ratio          5             1  DRLTrainer   \n",
       "2         compute_reward_from_tutor          5             1  DRLTrainer   \n",
       "3                     reward_profit          5             1  DRLTrainer   \n",
       "4               reward_sharpe_ratio          5             4  DQNTrainer   \n",
       "5  reward_differential_sharpe_ratio          5             4  DQNTrainer   \n",
       "6         compute_reward_from_tutor          5             4  DQNTrainer   \n",
       "7                     reward_profit          5             4  DQNTrainer   \n",
       "\n",
       "  from_checkpoint  \n",
       "0            None  \n",
       "1            None  \n",
       "2            None  \n",
       "3            None  \n",
       "4            None  \n",
       "5            None  \n",
       "6            None  \n",
       "7            None  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(dicts_list)\n",
    "print(df.shape)\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['action_domain', 'algorithm', 'asset', 'batch_size', 'data_source',\n",
       "       'episodes', 'loss_method', 'epoch', 'epsilon', 'epsilon_decay',\n",
       "       'epsilon_final', 'fee', 'gamma', 'learning_rate', 'money', 'obs_space',\n",
       "       'reward_function', 'runs_p_eps', 'action_space', 'trainer',\n",
       "       'from_checkpoint'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward_function</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward_sharpe_ratio</td>\n",
       "      <td>DRL_trial_0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reward_differential_sharpe_ratio</td>\n",
       "      <td>DRL_trial_0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>compute_reward_from_tutor</td>\n",
       "      <td>DRL_trial_0</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reward_profit</td>\n",
       "      <td>DRL_trial_8</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reward_sharpe_ratio</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reward_differential_sharpe_ratio</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>compute_reward_from_tutor</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reward_profit</td>\n",
       "      <td>DQN_trial_0</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    reward_function    algorithm  learning_rate  batch_size\n",
       "0               reward_sharpe_ratio  DRL_trial_0   1.000000e-07          16\n",
       "1  reward_differential_sharpe_ratio  DRL_trial_0   1.000000e-07          16\n",
       "2         compute_reward_from_tutor  DRL_trial_0   1.000000e-06          16\n",
       "3                     reward_profit  DRL_trial_8   1.000000e-05          32\n",
       "4               reward_sharpe_ratio  DQN_trial_0   1.000000e-07          16\n",
       "5  reward_differential_sharpe_ratio  DQN_trial_0   1.000000e-07          16\n",
       "6         compute_reward_from_tutor  DQN_trial_0   1.000000e-03          16\n",
       "7                     reward_profit  DQN_trial_0   1.000000e-05          16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['reward_function', 'algorithm', 'learning_rate', 'batch_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_domain': {0: [0.0, 1.0],\n",
      "                   1: [0.0, 1.0],\n",
      "                   2: [0.0, 1.0],\n",
      "                   3: [0.0, 1.0],\n",
      "                   4: [0.0, 1.0],\n",
      "                   5: [0.0, 1.0],\n",
      "                   6: [0.0, 1.0],\n",
      "                   7: [0.0, 1.0]},\n",
      " 'action_space': {0: 1, 1: 1, 2: 1, 3: 1, 4: 4, 5: 4, 6: 4, 7: 4},\n",
      " 'algorithm': {0: 'DRL_trial_0',\n",
      "               1: 'DRL_trial_0',\n",
      "               2: 'DRL_trial_0',\n",
      "               3: 'DRL_trial_8',\n",
      "               4: 'DQN_trial_0',\n",
      "               5: 'DQN_trial_0',\n",
      "               6: 'DQN_trial_0',\n",
      "               7: 'DQN_trial_0'},\n",
      " 'asset': {0: 'BTC',\n",
      "           1: 'BTC',\n",
      "           2: 'BTC',\n",
      "           3: 'BTC',\n",
      "           4: 'BTC',\n",
      "           5: 'BTC',\n",
      "           6: 'BTC',\n",
      "           7: 'BTC'},\n",
      " 'batch_size': {0: 16, 1: 16, 2: 16, 3: 32, 4: 16, 5: 16, 6: 16, 7: 16},\n",
      " 'data_source': {0: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 1: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 2: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 3: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 4: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 5: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 6: 'BTC_histData_dt1800.0s_20220825_0629',\n",
      "                 7: 'BTC_histData_dt1800.0s_20220825_0629'},\n",
      " 'episodes': {0: 50, 1: 50, 2: 50, 3: 50, 4: 50, 5: 50, 6: 50, 7: 50},\n",
      " 'epoch': {0: 2, 1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2},\n",
      " 'epsilon': {0: 0.5, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5},\n",
      " 'epsilon_decay': {0: 0.75,\n",
      "                   1: 0.75,\n",
      "                   2: 0.75,\n",
      "                   3: 0.75,\n",
      "                   4: 0.75,\n",
      "                   5: 0.75,\n",
      "                   6: 0.75,\n",
      "                   7: 0.75},\n",
      " 'epsilon_final': {0: 0.01,\n",
      "                   1: 0.01,\n",
      "                   2: 0.01,\n",
      "                   3: 0.01,\n",
      "                   4: 0.01,\n",
      "                   5: 0.01,\n",
      "                   6: 0.01,\n",
      "                   7: 0.01},\n",
      " 'fee': {0: 0.001,\n",
      "         1: 0.001,\n",
      "         2: 0.001,\n",
      "         3: 0.001,\n",
      "         4: 0.001,\n",
      "         5: 0.001,\n",
      "         6: 0.001,\n",
      "         7: 0.001},\n",
      " 'from_checkpoint': {0: None,\n",
      "                     1: None,\n",
      "                     2: None,\n",
      "                     3: None,\n",
      "                     4: None,\n",
      "                     5: None,\n",
      "                     6: None,\n",
      "                     7: None},\n",
      " 'gamma': {0: 0.95,\n",
      "           1: 0.95,\n",
      "           2: 0.95,\n",
      "           3: 0.95,\n",
      "           4: 0.95,\n",
      "           5: 0.95,\n",
      "           6: 0.95,\n",
      "           7: 0.95},\n",
      " 'learning_rate': {0: 1e-07,\n",
      "                   1: 1e-07,\n",
      "                   2: 1e-06,\n",
      "                   3: 1e-05,\n",
      "                   4: 1e-07,\n",
      "                   5: 1e-07,\n",
      "                   6: 0.001,\n",
      "                   7: 1e-05},\n",
      " 'loss_method': {0: 'stantard_loss',\n",
      "                 1: 'stantard_loss',\n",
      "                 2: 'stantard_loss',\n",
      "                 3: 'stantard_loss',\n",
      "                 4: 'stantard_loss',\n",
      "                 5: 'stantard_loss',\n",
      "                 6: 'stantard_loss',\n",
      "                 7: 'stantard_loss'},\n",
      " 'money': {0: 10000,\n",
      "           1: 10000,\n",
      "           2: 10000,\n",
      "           3: 10000,\n",
      "           4: 10000,\n",
      "           5: 10000,\n",
      "           6: 10000,\n",
      "           7: 10000},\n",
      " 'obs_space': {0: [8, 20],\n",
      "               1: [8, 20],\n",
      "               2: [8, 20],\n",
      "               3: [8, 20],\n",
      "               4: [8, 20],\n",
      "               5: [8, 20],\n",
      "               6: [8, 20],\n",
      "               7: [8, 20]},\n",
      " 'reward_function': {0: 'reward_sharpe_ratio',\n",
      "                     1: 'reward_differential_sharpe_ratio',\n",
      "                     2: 'compute_reward_from_tutor',\n",
      "                     3: 'reward_profit',\n",
      "                     4: 'reward_sharpe_ratio',\n",
      "                     5: 'reward_differential_sharpe_ratio',\n",
      "                     6: 'compute_reward_from_tutor',\n",
      "                     7: 'reward_profit'},\n",
      " 'runs_p_eps': {0: 5, 1: 5, 2: 5, 3: 5, 4: 5, 5: 5, 6: 5, 7: 5},\n",
      " 'trainer': {0: 'DRLTrainer',\n",
      "             1: 'DRLTrainer',\n",
      "             2: 'DRLTrainer',\n",
      "             3: 'DRLTrainer',\n",
      "             4: 'DQNTrainer',\n",
      "             5: 'DQNTrainer',\n",
      "             6: 'DQNTrainer',\n",
      "             7: 'DQNTrainer'}}\n"
     ]
    }
   ],
   "source": [
    "params = df.to_dict()\n",
    "pprint.pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_domain': [(0.0, 1.0)],\n",
      " 'action_space': [1, 4],\n",
      " 'algorithm': ['DRL_trial_8', 'DQN_trial_0', 'DRL_trial_0'],\n",
      " 'asset': ['BTC'],\n",
      " 'batch_size': [16, 32],\n",
      " 'data_source': ['BTC_histData_dt1800.0s_20220825_0629'],\n",
      " 'episodes': [50],\n",
      " 'epoch': [2],\n",
      " 'epsilon': [0.5],\n",
      " 'epsilon_decay': [0.75],\n",
      " 'epsilon_final': [0.01],\n",
      " 'fee': [0.001],\n",
      " 'from_checkpoint': [None],\n",
      " 'gamma': [0.95],\n",
      " 'learning_rate': [1e-05, 1e-07, 1e-06, 0.001],\n",
      " 'loss_method': ['stantard_loss'],\n",
      " 'money': [10000],\n",
      " 'obs_space': [(8, 20)],\n",
      " 'reward_function': ['reward_differential_sharpe_ratio',\n",
      "                     'reward_profit',\n",
      "                     'reward_sharpe_ratio',\n",
      "                     'compute_reward_from_tutor'],\n",
      " 'runs_p_eps': [5],\n",
      " 'trainer': ['DRLTrainer', 'DQNTrainer']}\n"
     ]
    }
   ],
   "source": [
    "clean_params = dict()\n",
    "for k,v in params.items():\n",
    "    # print(isinstance(v, dict))\n",
    "    # print(v.values())\n",
    "    if isinstance(v, dict):\n",
    "        t1 = [x for x in v.values()]\n",
    "        if isinstance(t1[0], list):\n",
    "            t1 = set(tuple(i) for i in t1)\n",
    "        tmp = list(set(t1))\n",
    "        clean_params[k] = tmp\n",
    "    else:\n",
    "        clean_params[k] = v\n",
    "\n",
    "\n",
    "pprint.pprint(clean_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
