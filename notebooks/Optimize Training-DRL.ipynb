{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c874392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:04:26.049338: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from env import BTCMarket_Env\n",
    "from agent import Trader_Agent\n",
    "from DQNtrainer import DQNTrainer\n",
    "from DRLtrainer import DRLTrainer\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24836622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent as TAgent\n",
    "import env as BTCEnv\n",
    "\n",
    "from importlib import reload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8457002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               41216     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,433\n",
      "Trainable params: 82,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Loss: <DRLtrainer.DRLLossFunctions object at 0x7f397d17b0a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:04:35.980599: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 16:04:35.981448: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "obs_space = (8,20)\n",
    "act_space = 1\n",
    "\n",
    "money = 20000\n",
    "fee = 0.001\n",
    "episodes = 1\n",
    "runs_p_eps = 1\n",
    "TAgent=reload(TAgent)\n",
    "BTCEnv=reload(BTCEnv)\n",
    "env = BTCEnv.BTCMarket_Env(observation_space = obs_space,\n",
    "            action_space = act_space,\n",
    "            start_money = money,\n",
    "            trading_fee= fee)\n",
    "agent = TAgent.Trader_Agent(observation_space = obs_space,\n",
    "            action_space = act_space,\n",
    "            epsilon = 0.01)\n",
    "drltrainer = DRLTrainer(env, agent,\n",
    "                observation_space = obs_space,\n",
    "                action_space = act_space,\n",
    "                batch_size=32,                        \n",
    "                lstm_path=\"./best_models/11_mar_2023/lstm_2.h5\")\n",
    "\n",
    "# drltrainer.rollout(n_episodes=episodes, run_per_episode=runs_p_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0300d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "data_samples = env.episode_length\n",
    "memory = deque()\n",
    "for t in range(20):\n",
    "    state, _, _ = env.step(np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9941e849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a0f61b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.52278113], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.predict(state, verbose=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fcbb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "data_samples = env.episode_length\n",
    "memory = deque()\n",
    "\n",
    "state, _, _ = env.step(np.array([0]))\n",
    "for t in tqdm(range(data_samples)):\n",
    "    # Compute Action\n",
    "    tmp_wallet_value = env.wallet_value\n",
    "    action = agent.compute_action(state)\n",
    "    # Compute new step\n",
    "    next_state, reward, done = env.step(action=action)\n",
    "    # save Experience to Memory\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7488498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22234.95])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.wallet_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c234b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[nan]\n",
      "[[0.53175]]\n",
      "[nan]\n",
      "[[0.48605838]]\n",
      "[nan]\n",
      "[[0.5748076]]\n",
      "[nan]\n",
      "[[0.5367304]]\n",
      "[nan]\n",
      "[[0.46627843]]\n",
      "[nan]\n",
      "[[0.45344213]]\n",
      "[nan]\n",
      "[[0.49437934]]\n",
      "[nan]\n",
      "[[0.42137742]]\n",
      "[nan]\n",
      "[[0.44183114]]\n",
      "[nan]\n",
      "[[0.49554604]]\n",
      "[nan]\n",
      "[[0.42949927]]\n",
      "[nan]\n",
      "[[0.41737896]]\n",
      "[nan]\n",
      "[[0.47609264]]\n",
      "[nan]\n",
      "[[0.44837612]]\n",
      "[nan]\n",
      "[[0.4503274]]\n",
      "[nan]\n",
      "[[0.5113878]]\n",
      "[nan]\n",
      "[[0.4983522]]\n",
      "[nan]\n",
      "[[0.47565052]]\n",
      "[nan]\n",
      "[[0.4914827]]\n",
      "[nan]\n",
      "[[0.41708475]]\n",
      "[nan]\n",
      "[[0.40020627]]\n",
      "[nan]\n",
      "[[0.43556508]]\n",
      "[nan]\n",
      "[[0.4912107]]\n",
      "[nan]\n",
      "[[0.52390206]]\n",
      "[nan]\n",
      "[[0.5288155]]\n",
      "[nan]\n",
      "[[0.51309913]]\n",
      "[nan]\n",
      "[[0.45242944]]\n",
      "[nan]\n",
      "[[0.4105394]]\n",
      "[nan]\n",
      "[[0.40095982]]\n",
      "[nan]\n",
      "[[0.32136416]]\n",
      "[nan]\n",
      "[[0.37901244]]\n",
      "[nan]\n",
      "[[0.42903683]]\n",
      "[nan]\n",
      "[[0.41149423]]\n",
      "[nan]\n",
      "[[0.37230483]]\n",
      "[nan]\n",
      "[[0.34529626]]\n",
      "[nan]\n",
      "[[0.4195807]]\n",
      "[nan]\n",
      "[[0.40654886]]\n",
      "[nan]\n",
      "[[0.4716751]]\n",
      "[nan]\n",
      "[[0.5065589]]\n",
      "[nan]\n",
      "[[0.431485]]\n",
      "[nan]\n",
      "[[0.5309753]]\n",
      "[nan]\n",
      "[[0.55155677]]\n",
      "[nan]\n",
      "[[0.43159816]]\n",
      "[nan]\n",
      "[[0.49637097]]\n",
      "[nan]\n",
      "[[0.4248774]]\n",
      "[nan]\n",
      "[[0.52509236]]\n",
      "[nan]\n",
      "[[0.4247751]]\n",
      "[nan]\n",
      "[[0.44242832]]\n",
      "[nan]\n",
      "[[0.4072118]]\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 1s 27ms/step - loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "\n",
    "batch = []\n",
    "# for i in range(1):\n",
    "for i in range(max([len(memory) - batch_size,0]), len(memory)):\n",
    "    batch.append(memory[i])\n",
    "\n",
    "# Change here to fit new RL-Algo\n",
    "# for index in range(0,len(batch)-1):\n",
    "\n",
    "x_train = np.zeros((batch_size, 20,8))\n",
    "y_train = np.zeros((batch_size, 4))\n",
    "print(len(batch))\n",
    "\n",
    "\n",
    "state, _, reward, next_state , done = batch[0]\n",
    "action = agent.model.predict(state,verbose = 0)\n",
    "for index in range(1,len(batch)):\n",
    "    # Compute Reward Decay for DQN\n",
    "    action_next = agent.model.predict(next_state,verbose = 0)\n",
    "#     if not done:\n",
    "#         reward += dqntrainer.gamma * np.max(action_next)\n",
    "        \n",
    "    if np.any(np.isnan(reward)) or np.any(np.isnan(action)):\n",
    "        print(reward)\n",
    "        print(action)\n",
    "    target = np.nan_to_num(action,nan=0.0)      \n",
    "    id_act = np.argmax(target)\n",
    "    target[0,id_act] = reward\n",
    "    \n",
    "    target = np.nan_to_num(target,nan=0.0)\n",
    "    state = np.nan_to_num(state,nan=0.0)\n",
    "    x_train[index]= state[0]\n",
    "    y_train[index]= target[0]\n",
    "    \n",
    "    \n",
    "    # update state, action, reward for training\n",
    "    state, _, reward, next_state , done = batch[index]\n",
    "    action = action_next\n",
    "\n",
    "\n",
    "result=drltrainer.agent.model.fit(x_train, y_train, epochs=5, verbose=1)\n",
    "# agent.update_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647de70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
